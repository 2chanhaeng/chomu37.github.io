---
layout: post
title: 활성화함수
description: >
  대표적인 활성화함수(Sigmoid, tanh, ReLU, SoftMax)에 대한 요약글
tags: 
---
# 활성화함수
## 1. **Sigmoid**
 $$\operatorname{\sigma}(x)=\frac1{1+e^{-x}}$$
 * 그래디언트 소실 문제 vanishing gradient problem:\
   그래디언트가 0이 됨으로 인해 발생하는 문제
 * 그래디언트 폭주 문제 exploding gradient problem:\
   그래디언트가 발산함으로 인해 발생하는 문제
 
 이와 같은 문제로 신경망에서 주로 출력층에서만 이용한다.
 치역이 $(0,1)$으로 출력을 확률로 압축하기 좋기 때문이다.
## 2. **tanh**: hyperbolic tangent
 $$\tanh x=\frac{e^x-e^{-x}}{e^x+e^{-x}}=2\operatorname{\sigma}(2x)-1$$
## 3. **ReLU**: Rectified Linear Unit
 $$\operatorname{ReLU}(x)=\max(0,x)$$
 음수 출력이 없어 그래디언트 소실 문제가 없다.
 그러나 신경망의 특정 출력이 0이 되면 다시는 돌아오지 않는다.(죽은 ReLU 문제)
 이런 현상을 방지하기 위하여 LeackyReLU, PReLU(Parametric ReLU) 등의 변형함수가 개발되었다.
 두 함수는 $\max(x,ax)\ (0<a<1)$ 꼴로 동일하나 다음과 같은 차이가 있다.
   * LeackyReLU: 누수 파라미터 $a$가 사전에 설정하여 변하지 않는 하이퍼 파라미터이다.
   * PReLU: 누수 파라미터 $a$가 학습 가능한 파라미터이다.
## 4. **SoftMax**
 $$\operatorname{SoftMax}(x_i) = \frac{e^{x_i}}{\displaystyle\sum_{k=1}^ne^{x_k}}$$
 모든 출력의 합으로 각 출력을 나누어 n개의 클래스에 대한 이산 확률 분포를 만든다. 따라서 출력의 총합은 1이 된다.
 보통 확률 기반의 목적 함수 '범주형 크로스 엔트로피'와 함께 사용한다.

출처: 파이토치로 배우는 자연어 처리(델립 라오 & 브라이언 맥머핸 지음, 박해선 옮김, 한빛미디어)